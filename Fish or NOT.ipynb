{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSet():\n",
    "    dataSet = [[1,1,'yes'],\n",
    "              [1,1,'yes'],\n",
    "              [1,0,'No'],\n",
    "              [0,1,'No'],\n",
    "              [0,1,'No']]\n",
    "    FeatureNames = ['No surfacing', 'flippers']\n",
    "    return dataSet, FeatureNames\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the data\n",
    "Here I use Entropy Gain as the method to select the features, which is called the ID3 algorithm, then data is required to be discrete. Here we have the discrete data already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyzing Data\n",
    "1. Using ID3 method, calculating the Entropy: H(X) = - SUM P(x)*log2P(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcEntropy(dataSet):\n",
    "    # how many entries\n",
    "    numEntries = len(dataSet)\n",
    "    # create a dict to store the label counts\n",
    "    labelCounts = {}\n",
    "    # iterate the dataset\n",
    "    for featureVector in dataSet:\n",
    "        # the last term in featureVecot is the label\n",
    "        label = featureVector[-1]\n",
    "        # if label not in labelCounts, then initialize it\n",
    "        if label not in labelCounts:\n",
    "            labelCounts[label] = 0\n",
    "        labelCounts[label] += 1\n",
    "    \n",
    "    # based on the label counts, calculate the Entropy\n",
    "    entropy = 0.0\n",
    "    for key in labelCounts:\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        entropy -= prob * log(prob, 2)\n",
    "    \n",
    "    # return the entropy for the dataset\n",
    "    return entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Spliting data, based on selected feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataSet(dataset, index, value):\n",
    "    \"\"\"\n",
    "    dataset: the given data set\n",
    "    index: the column index that is selected feature to split the data\n",
    "    value: the target value\n",
    "    \"\"\"\n",
    "    dataset_new =[]\n",
    "    for record in dataset:\n",
    "        if record[index] == value:\n",
    "            record_new = record[:index]\n",
    "            record_new.extend(record[index+1:])\n",
    "            dataset_new.append(record_new)\n",
    "    return dataset_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Based on the previous fucntion, we want to find the feature that gives the largest information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindBestFeature(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1\n",
    "    baseEntropy = calcEntropy(dataset)\n",
    "    maxInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    for index in range(numFeatures):\n",
    "        subEntropy = 0.0\n",
    "        values = set([record[index] for record in dataset])\n",
    "        for value in values:\n",
    "            dataset_sub = splitDataSet(dataset, index, value)\n",
    "            prob = len(dataset_sub) / numFeatures\n",
    "            subEntropy -= calcEntropy(dataset_sub)*prob\n",
    "        infoGain = baseEntropy - subEntropy\n",
    "    \n",
    "        if (infoGain > maxInfoGain):\n",
    "            maxInfoGain = infoGain\n",
    "            bestFeature = index\n",
    "        \n",
    "    print('Info Gain is: ', infoGain, 'Best Feature index is: ', bestFeature, baseEntropy, subEntropy)\n",
    "    \n",
    "    return bestFeature\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataset, FeatureNames):\n",
    "    labelList = [record[-1] for record in dataset]\n",
    "    # there are two stop criterion \n",
    "    # 1. when there is no other class lable\n",
    "    if labelList.count(labelList[0]) == len(labelList):\n",
    "        return labelList[0]\n",
    "    # 2. when all features are used but there are more than one labels in the dataset\n",
    "    if len(dataset[0]) == 1:\n",
    "        return majorityLabel(labelList)\n",
    "    \n",
    "    # otherwise, we have to find the best feature to split the data set\n",
    "    bestFeatureIndex = FindBestFeature(dataset)\n",
    "    bestFeatureName = FeatureNames[bestFeatureIndex]\n",
    "    \n",
    "    # initialize the tree\n",
    "    myTree = {bestFeatureName:{}}\n",
    "    del(FeatureNames[bestFeatureIndex])\n",
    "    \n",
    "    # create sub trees for this feature with different values\n",
    "    featureValues = [record[bestFeatureIndex] for record in dataset]\n",
    "    featureValues = set(featureValues)\n",
    "    for value in featureValues:\n",
    "        subFeatures = FeatureNames[:]    \n",
    "        myTree[bestFeatureName][value] = createTree(splitDataSet(dataset, bestFeatureIndex, value), subFeatures)\n",
    "    \n",
    "    return myTree\n",
    "    \n",
    "def majorityLabel(labelList):\n",
    "    LableCount = {}\n",
    "    for label in labelList:\n",
    "        if label not in labelCount:\n",
    "            labelCount[label] = 0\n",
    "        labelCount[count] += 1\n",
    "    labelCount_sorted = sorted(classCount.iteritms(), key=operator.itemgetter(1), reverse = True)\n",
    "    \n",
    "    return labelCount_sorted[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Decision Tree to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree, featName, TestVec):\n",
    "    \"\"\"\n",
    "    inputTree\n",
    "    featName\n",
    "    TestVec\n",
    "    \"\"\"\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    secondDict = inputTree[firstStr]    \n",
    "    featIndex = featName.index(firstStr)\n",
    "    key = TestVec[featIndex]\n",
    "    valueOfFeat = secondDict[key]\n",
    "    \n",
    "    print('+++', firstStr, 'xxx', secondDict, '---', key, '>>>', valueOfFeat)\n",
    "    # check whether valueOfFeat is dict or not\n",
    "    if isinstance(valueOfFeat, dict):\n",
    "        classLabel = classify(valueOfFeat, featIndex, TestVec)\n",
    "    else:\n",
    "        classLabel = valueOfFeat\n",
    "    return classLabel\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeTree(inputTree, filename):\n",
    "    import pickle\n",
    "    # -------------- 第一种方法 start --------------\n",
    "    fw = open(filename, 'wb')\n",
    "    pickle.dump(inputTree, fw)\n",
    "    fw.close()\n",
    "    \n",
    "def grabTree(filename):\n",
    "    import pickle\n",
    "    fr = open(filename,'rb')\n",
    "    return pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info Gain is:  2.970950594454669 Best Feature index is:  1 0.9709505944546686 -2.0\n",
      "Info Gain is:  1.0 Best Feature index is:  0 1.0 0.0\n",
      ".... {'flippers': {0: 'No', 1: {'No surfacing': {0: 'No', 1: 'yes'}}}}\n",
      "['No surfacing', 'flippers']\n",
      "+++ flippers xxx {0: 'No', 1: {'No surfacing': {0: 'No', 1: 'yes'}}} --- 1 >>> {'No surfacing': {0: 'No', 1: 'yes'}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-59af8c2e38da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"....\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmyTree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyTree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#print(get_tree_height(myTree))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-6084366b1d1d>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(inputTree, featName, TestVec)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# check whether valueOfFeat is dict or not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalueOfFeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mclassLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalueOfFeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTestVec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mclassLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalueOfFeat\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-6084366b1d1d>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(inputTree, featName, TestVec)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mfirstStr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0msecondDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputTree\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirstStr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mfeatIndex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirstStr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestVec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mvalueOfFeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msecondDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "myDat, labels = createDataSet()\n",
    "\n",
    "myTree = createTree(myDat, copy.deepcopy(labels))\n",
    "print(\"....\",myTree)\n",
    "print(labels)\n",
    "print(classify(myTree, labels, [1, 1]))\n",
    "\n",
    "#print(get_tree_height(myTree))\n",
    "\n",
    "#dtPlot.createPlot(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(myTree.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.index(list(myTree.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
